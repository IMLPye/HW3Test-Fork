{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Information:\n",
    "\n",
    "### Team Member 1:\n",
    "* UNI:  yo2258\n",
    "* Name:Ye Ouyang\n",
    "\n",
    "### Team Member 2 [optional]:\n",
    "* UNI:  \n",
    "* Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0 - Import Libraries, Load Data [0 points]\n",
    "\n",
    "This is the basic step where you can load the data and create train and test sets for internal validation as per your convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dataPath = \"data/data.csv\"\n",
    "data = pd.read_csv(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1 - Exploration and Preparation [10 points]\n",
    "\n",
    "In this step, we expect you to look into the data and try to understand it before modeling. This understanding may lead to some basic data preparation steps which are common across the two model sets required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write code below, y\n",
    "data = pd.get_dummies(data)\n",
    "X = data.iloc[:,:-2]\n",
    "y = data.iloc[:,-1:]    \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 - ModelSet1 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set1:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. Any classification algorithm covered in class apart from tree-based models can be tested here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelset1():    \n",
    "    # 1: Nearest Centroid    \n",
    "    nc = NearestCentroid().fit(X_train, y_train)\n",
    "    print (nc.score(X_test,y_test))\n",
    "    \n",
    "    # 2: KNN  \n",
    "    knns = neighbors.KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train)\n",
    "    print (knns.score(X_test,y_test))\n",
    "    \n",
    "    # 3: SVM      \n",
    "    linearsvm = SVC().fit(X_train,y_train)\n",
    "    print (linearsvm.score(X_test,y_test))\n",
    "    \n",
    "    # 4: SGD      \n",
    "    clf = SGDClassifier(loss=\"hinge\").fit(X_train,y_train)\n",
    "    print (clf.score(X_test,y_test))\n",
    "    \n",
    "    # 5: LogReg  \n",
    "    logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\").fit(X_train,y_train)\n",
    "    print (logreg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 - ModelSet2 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set2:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. We encourage you to try decition tree, random forest and gradient boosted tree methods here and pick the one which you think works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelset2():    \n",
    "    # 1: Decision Tree\n",
    "    dt = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "    dt.score(X_test,y_test)\n",
    "    \n",
    "    # 2: Random Forest\n",
    "    rf = RandomForestClassifier().fit(X_train,y_train)\n",
    "    rf.score(X_test,y_test)\n",
    "    \n",
    "    # 3: Gradient boosted Tree\n",
    "    gbc = GradientBoostingClassifier().fit(X_train,y_train)\n",
    "    gbc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 - Ensemble [20 points + 10 Bonus points]\n",
    "\n",
    "In this step, we expect you to use the models created before and create new predictions. You should definitely try poor man's stacking but we encourage you to think of different ensemble techniques as well. We will judge your creativity and improvement in model performance using ensemble models and you can potentially earn 10 bonus points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "    # Ensemble Clustering\n",
    "    voting = VotingClassifier([('logreg',LogisticRegression(C=100)),\n",
    "                               ('tree',DecisionTreeClassifier()),\n",
    "                               ('knn',KNeighborsClassifier())]\n",
    "                              ,voting='soft')\n",
    "    model = voting.fit(X_train,y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print model.score(X_test,y_test) \n",
    "    \n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "    plt.title('ROC for Ensemble Classification')\n",
    "    plt.plot(false_positive_rate, true_positive_rate, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.2])\n",
    "    plt.ylim([-0.1,1.2])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    return voting.score(X_test,y_test)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_accuracy():\n",
    "    assert(ensemble()>0.65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
